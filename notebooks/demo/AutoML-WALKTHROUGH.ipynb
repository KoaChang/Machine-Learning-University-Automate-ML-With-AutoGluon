{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-image: linear-gradient(145deg, rgba(35, 47, 62, 1) 0%, rgba(0, 49, 129, 1) 40%, rgba(32, 116, 213, 1) 60%, rgba(244, 110, 197, 1) 85%, rgba(255, 173, 151, 1) 100%); padding: 1rem 2rem;\n",
    "\"><img src=\"https://cdn-prod.mlu.aws.dev/static/amazon_apollo_django_setup_staging/da021f332105bfea6edc2b02f78330ab1e750dfb01896a80b9676a49743759a4/img/mlu_logo.png\" class=\"logo\" alt=\"MLU Logo\"></div>\n",
    "\n",
    "# <a name=\"0\">Code Walkthrough & Advanced AutoGluon Features</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use AutoGluon `TabularPredictor` to solve two machine learning tasks: a __regression task__ (book price prediction) and a __multiclass classification task__ (occupation prediction). \n",
    "\n",
    "<a href=\"#01\">Part I - Solution Walkthrough & Discussions</a>, covers a basic solution for the Book Price regression problem from the *MLU-DAY-ONE-ML-Hands-On.ipynb* notebook.\n",
    "\n",
    "<a href=\"#02\">Part II - Advanced AutoGluon Features</a>, dives deeper into more advanced AutoGluon features, solving a multiclass classification task of predicting the occupation of individuals using US census data.\n",
    "\n",
    "- Part II - 1. <a href=\"#1\">ML Problem Description</a>\n",
    "- Part II - 2. <a href=\"#2\">Loading the Data</a>\n",
    "- Part II - 3. <a href=\"#5\">Model Training with AutoGluon</a>\n",
    "- Part II - 4. <a href=\"#7\">Model ensembling with stacking/bagging</a>\n",
    "- Part II - 5. <a href=\"#8\">Prediction options (inference)</a>\n",
    "- Part II - 6. <a href=\"#10\">Selecting individual models for predictions</a>\n",
    "- Part II - 7. <a href=\"#11\">Interpretability: Feature importance</a>\n",
    "- Part II - 8. <a href=\"#12\">Inference Speed: Model \n",
    "- Part II - 9. <a href=\"#13\">Before You Go (clean up model artifacts)</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load in libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the libraries needed to work with our Tabular dataset.\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset\n",
    "\n",
    "# Additional library for tuning\n",
    "import autogluon.core as ag\n",
    "from autogluon.common import space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <a name=\"01\">Part I - Walkthrough & Discussions</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "The shortest possible solution for the hands-on activity is shown below. You start by loading in the datasets (train and test), then train the predictor and use it to create predictions for the test dataset. The final three lines of code create a CSV (this is only required when you want to save your results to a file; otherwhise you could look at the predictions directly in your coding environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240106_025953\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240106_025953\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Wed Sep 6 21:15:41 UTC 2023\n",
      "CPU Count:          4\n",
      "Memory Avail:       11.94 GB / 15.32 GB (77.9%)\n",
      "Disk Space Avail:   45.37 GB / 49.04 GB (92.5%)\n",
      "===================================================\n",
      "Train Data Rows:    5051\n",
      "Train Data Columns: 9\n",
      "Label Column:       Price\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (4.149249912590282, 1.414973347970818, 2.60147, 0.33003)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    12240.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 10.40 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['Title', 'Edition', 'Ratings', 'Synopsis']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 4920\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])          : 1 | ['ID']\n",
      "\t\t('object', [])       : 4 | ['Author', 'Reviews', 'Genre', 'BookCategory']\n",
      "\t\t('object', ['text']) : 4 | ['Title', 'Edition', 'Ratings', 'Synopsis']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :    4 | ['Author', 'Reviews', 'Genre', 'BookCategory']\n",
      "\t\t('category', ['text_as_category'])  :    4 | ['Title', 'Edition', 'Ratings', 'Synopsis']\n",
      "\t\t('int', [])                         :    1 | ['ID']\n",
      "\t\t('int', ['binned', 'text_special']) :   55 | ['Title.char_count', 'Title.word_count', 'Title.capital_ratio', 'Title.lower_ratio', 'Title.digit_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 4859 | ['__nlp__.000', '__nlp__.10', '__nlp__.10 customer', '__nlp__.100', '__nlp__.11', ...]\n",
      "\t24.9s = Fit runtime\n",
      "\t9 features in original data used to generate 4923 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 47.19 MB (0.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 25.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 4545, Val Rows: 506\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 274.35s of the 274.2s of remaining time.\n",
      "\t-0.1243\t = Validation score   (-mean_squared_error)\n",
      "\t1.52s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 272.18s of the 272.03s of remaining time.\n",
      "\t-0.1232\t = Validation score   (-mean_squared_error)\n",
      "\t1.51s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 269.99s of the 269.81s of remaining time.\n",
      "\t-0.0457\t = Validation score   (-mean_squared_error)\n",
      "\t10.97s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 258.72s of the 258.47s of remaining time.\n",
      "\t-0.0457\t = Validation score   (-mean_squared_error)\n",
      "\t8.98s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 249.41s of the 249.15s of remaining time.\n",
      "\t-0.0557\t = Validation score   (-mean_squared_error)\n",
      "\t510.57s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 274.35s of the -263.08s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.464, 'LightGBM': 0.429, 'RandomForestMSE': 0.107}\n",
      "\t-0.0444\t = Validation score   (-mean_squared_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 563.55s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240106_025953\")\n"
     ]
    }
   ],
   "source": [
    "# Loading the train and test datasets\n",
    "df_train = TabularDataset(\"../../data/training.csv\")\n",
    "df_test = TabularDataset(\"../../data/mlu-leaderboard-test.csv\")\n",
    "\n",
    "# Train a predictor with AutoGluon on the train dataset\n",
    "predictor = TabularPredictor(label=\"Price\", eval_metric=\"mean_squared_error\").fit(\n",
    "    train_data=df_train, time_limit = 5 * 60\n",
    ")\n",
    "\n",
    "# Make predictions on the test dataset with the AutoGluon model\n",
    "predictions = predictor.predict(df_test)\n",
    "\n",
    "# Creating a new dataframe for the MLU Leaderboard submission\n",
    "submission = df_test[[\"ID\"]].copy(deep=True)\n",
    "\n",
    "# Creating label column from price prediction list\n",
    "submission[\"Price\"] = predictions\n",
    "\n",
    "# Save the dataframe as a csv file for MLU Leaderboard submission\n",
    "# index=False prevents printing the row IDs as separate values\n",
    "submission.to_csv(\n",
    "    \"../../data/predictions/Solution-Demo.csv\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <a name=\"02\">Part II - Advanced AutoGluon Features</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this section we will look at some advanced features of AutoGluon. We will also start using a new dataset so that you can see what another type of common ML problem looks like: classification.\n",
    "\n",
    "## <a name=\"1\">ML Problem Description</a>\n",
    "\n",
    "Predict the occupation of individuals using census data. \n",
    "> This is a __multiclass classification__ task (15 distinct classes). <br>\n",
    "\n",
    "For the advanced feature demonstration we use a new dataset: Census data. In this particular dataset, each row corresponds to an individual person, and the columns contain various demographic characteristics collected for the census.\n",
    "\n",
    "We predict the occupation of an individual - this is a multiclass classification problem. Start by importing AutoGluon’s `TabularPredictor` and `TabularDataset`, and load the data from a S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"2\">Loading the data</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv | Columns = 15 / 15 | Rows = 39073 -> 39073\n",
      "Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv | Columns = 15 / 15 | Rows = 9769 -> 9769\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6118</th>\n",
       "      <td>51</td>\n",
       "      <td>Private</td>\n",
       "      <td>39264</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23204</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>51662</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29590</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>326310</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18116</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>222450</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Sales</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>2339</td>\n",
       "      <td>40</td>\n",
       "      <td>El-Salvador</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33964</th>\n",
       "      <td>62</td>\n",
       "      <td>Private</td>\n",
       "      <td>109190</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       age workclass  fnlwgt      education  education-num  \\\n",
       "6118    51   Private   39264   Some-college             10   \n",
       "23204   58   Private   51662           10th              6   \n",
       "29590   40   Private  326310   Some-college             10   \n",
       "18116   37   Private  222450        HS-grad              9   \n",
       "33964   62   Private  109190      Bachelors             13   \n",
       "\n",
       "            marital-status        occupation    relationship    race      sex  \\\n",
       "6118    Married-civ-spouse   Exec-managerial            Wife   White   Female   \n",
       "23204   Married-civ-spouse     Other-service            Wife   White   Female   \n",
       "29590   Married-civ-spouse      Craft-repair         Husband   White     Male   \n",
       "18116        Never-married             Sales   Not-in-family   White     Male   \n",
       "33964   Married-civ-spouse   Exec-managerial         Husband   White     Male   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week  native-country   class  \n",
       "6118              0             0              40   United-States    >50K  \n",
       "23204             0             0               8   United-States   <=50K  \n",
       "29590             0             0              44   United-States   <=50K  \n",
       "18116             0          2339              40     El-Salvador   <=50K  \n",
       "33964         15024             0              40   United-States    >50K  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the dataset\n",
    "train_data = TabularDataset(\"https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv\")\n",
    "\n",
    "# Let's load the test data\n",
    "test_data = TabularDataset(\"https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv\")\n",
    "\n",
    "# Subsample a subset of data for faster demo, try setting this to much larger values\n",
    "subsample_size = 1000\n",
    "\n",
    "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a name=\"5\">Model Training with AutoGluon</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We specify eval-metric just for demo (unnecessary as it's the default)\n",
    "metric = \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full list of AutoGluon classification metrics can be found here:\n",
    "\n",
    "`'accuracy', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_weighted', 'roc_auc', 'average_precision', 'precision', 'precision_macro', 'precision_micro', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_weighted', 'log_loss', 'pac_score'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying settings for TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train various models for ~2 min\n",
    "time_limit = 2 * 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying hyperparameters and tuning them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_options = {  # specifies non-default hyperparameter values for neural network models\n",
    "    'num_epochs': 10,  # number of training epochs (controls training time of NN models)\n",
    "    'learning_rate': space.Real(1e-4, 1e-2, default=5e-4, log=True),  # learning rate used in training (real-valued hyperparameter searched on log-scale)\n",
    "    'activation': space.Categorical('relu', 'softrelu', 'tanh'),  # activation function used in NN (categorical hyperparameter, default = first entry)\n",
    "    'dropout_prob': space.Real(0.0, 0.5, default=0.1),  # dropout probability (real-valued hyperparameter)\n",
    "}\n",
    "\n",
    "gbm_options = {  # specifies non-default hyperparameter values for lightGBM gradient boosted trees\n",
    "    'num_boost_round': 100,  # number of boosting rounds (controls training time of GBM models)\n",
    "    'num_leaves': space.Int(lower=26, upper=66, default=36),  # number of leaves in trees (integer hyperparameter)\n",
    "}\n",
    "\n",
    "hyperparameters = {  # hyperparameters of each model type\n",
    "                   'GBM': gbm_options,\n",
    "                   'NN_TORCH': nn_options,  # NOTE: comment this line out if you get errors on Mac OSX\n",
    "                  }  # When these keys are missing from hyperparameters dict, no models of that type are trained\n",
    "\n",
    "num_trials = 5  # try at most 5 different hyperparameter configurations for each type of model\n",
    "search_strategy = 'auto'  # to tune hyperparameters using random search routine with a local scheduler\n",
    "\n",
    "hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "    'num_trials': num_trials,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher': search_strategy,\n",
    "}  # Refer to TabularPredictor.fit docstring for all valid values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitted model: NeuralNetTorch/1ba3519e ...\n",
      "\t0.355\t = Validation score   (accuracy)\n",
      "\t2.48s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitted model: NeuralNetTorch/4b034927 ...\n",
      "\t0.355\t = Validation score   (accuracy)\n",
      "\t4.58s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitted model: NeuralNetTorch/2ed34b4b ...\n",
      "\t0.23\t = Validation score   (accuracy)\n",
      "\t2.29s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitted model: NeuralNetTorch/e38e9845 ...\n",
      "\t0.37\t = Validation score   (accuracy)\n",
      "\t2.53s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitted model: NeuralNetTorch/d61e8ac0 ...\n",
      "\t0.355\t = Validation score   (accuracy)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 119.87s of the 99.96s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch/e38e9845': 0.833, 'NeuralNetTorch/4b034927': 0.167}\n",
      "\t0.38\t = Validation score   (accuracy)\n",
      "\t0.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 20.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240106_030919\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=\"occupation\", eval_metric=metric).fit(\n",
    "    train_data,\n",
    "    time_limit=time_limit,\n",
    "    hyperparameters=hyperparameters,\n",
    "    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following to view a summary of what happened during the fit. Now this command will show details of the hyperparameter-tuning process for each type of model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                     model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L2      0.380    accuracy       0.040907  7.487318                0.000742           0.379155            2       True          6\n",
      "1  NeuralNetTorch/e38e9845      0.370    accuracy       0.023070  2.529382                0.023070           2.529382            1       True          4\n",
      "2  NeuralNetTorch/d61e8ac0      0.355    accuracy       0.011454  0.857236                0.011454           0.857236            1       True          5\n",
      "3  NeuralNetTorch/1ba3519e      0.355    accuracy       0.016551  2.475698                0.016551           2.475698            1       True          1\n",
      "4  NeuralNetTorch/4b034927      0.355    accuracy       0.017095  4.578781                0.017095           4.578781            1       True          2\n",
      "5  NeuralNetTorch/2ed34b4b      0.230    accuracy       0.018821  2.287487                0.018821           2.287487            1       True          3\n",
      "Number of models trained: 6\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "('int', ['bool']) : 2 | ['sex', 'class']\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20240106_030919SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'NeuralNetTorch/1ba3519e': 'TabularNeuralNetTorchModel',\n",
       "  'NeuralNetTorch/4b034927': 'TabularNeuralNetTorchModel',\n",
       "  'NeuralNetTorch/2ed34b4b': 'TabularNeuralNetTorchModel',\n",
       "  'NeuralNetTorch/e38e9845': 'TabularNeuralNetTorchModel',\n",
       "  'NeuralNetTorch/d61e8ac0': 'TabularNeuralNetTorchModel',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'NeuralNetTorch/1ba3519e': 0.355,\n",
       "  'NeuralNetTorch/4b034927': 0.355,\n",
       "  'NeuralNetTorch/2ed34b4b': 0.23,\n",
       "  'NeuralNetTorch/e38e9845': 0.37,\n",
       "  'NeuralNetTorch/d61e8ac0': 0.355,\n",
       "  'WeightedEnsemble_L2': 0.38},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'NeuralNetTorch/1ba3519e': ['NeuralNetTorch', '1ba3519e'],\n",
       "  'NeuralNetTorch/4b034927': ['NeuralNetTorch', '4b034927'],\n",
       "  'NeuralNetTorch/2ed34b4b': ['NeuralNetTorch', '2ed34b4b'],\n",
       "  'NeuralNetTorch/e38e9845': ['NeuralNetTorch', 'e38e9845'],\n",
       "  'NeuralNetTorch/d61e8ac0': ['NeuralNetTorch', 'd61e8ac0'],\n",
       "  'WeightedEnsemble_L2': ['WeightedEnsemble_L2']},\n",
       " 'model_fit_times': {'NeuralNetTorch/1ba3519e': 2.4756977558135986,\n",
       "  'NeuralNetTorch/4b034927': 4.5787811279296875,\n",
       "  'NeuralNetTorch/2ed34b4b': 2.287487268447876,\n",
       "  'NeuralNetTorch/e38e9845': 2.5293819904327393,\n",
       "  'NeuralNetTorch/d61e8ac0': 0.8572361469268799,\n",
       "  'WeightedEnsemble_L2': 0.37915468215942383},\n",
       " 'model_pred_times': {'NeuralNetTorch/1ba3519e': 0.016550779342651367,\n",
       "  'NeuralNetTorch/4b034927': 0.017095088958740234,\n",
       "  'NeuralNetTorch/2ed34b4b': 0.018820762634277344,\n",
       "  'NeuralNetTorch/e38e9845': 0.023070096969604492,\n",
       "  'NeuralNetTorch/d61e8ac0': 0.011453866958618164,\n",
       "  'WeightedEnsemble_L2': 0.0007419586181640625},\n",
       " 'num_bag_folds': 0,\n",
       " 'max_stack_level': 2,\n",
       " 'num_classes': 13,\n",
       " 'model_hyperparams': {'NeuralNetTorch/1ba3519e': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 1.0,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.1,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0005,\n",
       "   'weight_decay': 1e-06,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 0.99,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 2,\n",
       "   'hidden_size': 128,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': False,\n",
       "   'loss_function': 'auto'},\n",
       "  'NeuralNetTorch/4b034927': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 1.1,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.14529176312805409,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0006856772405140876,\n",
       "   'weight_decay': 0.0007733841899787831,\n",
       "   'proc.embed_min_categories': 4,\n",
       "   'proc.impute_strategy': 'most_frequent',\n",
       "   'proc.max_category_levels': 300,\n",
       "   'proc.skew_threshold': 10.0,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 2,\n",
       "   'hidden_size': 256,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': True,\n",
       "   'loss_function': 'auto'},\n",
       "  'NeuralNetTorch/2ed34b4b': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 0.8,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.09725215131712472,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.00015073602457331618,\n",
       "   'weight_decay': 1.0691216581982488e-08,\n",
       "   'proc.embed_min_categories': 1000,\n",
       "   'proc.impute_strategy': 'median',\n",
       "   'proc.max_category_levels': 300,\n",
       "   'proc.skew_threshold': 0.9,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 4,\n",
       "   'hidden_size': 128,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': False,\n",
       "   'loss_function': 'auto'},\n",
       "  'NeuralNetTorch/e38e9845': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 0.6,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.4278038023202308,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.0013080223132896777,\n",
       "   'weight_decay': 0.0002011029701432853,\n",
       "   'proc.embed_min_categories': 1000,\n",
       "   'proc.impute_strategy': 'mean',\n",
       "   'proc.max_category_levels': 10,\n",
       "   'proc.skew_threshold': 0.2,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 3,\n",
       "   'hidden_size': 512,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': False,\n",
       "   'loss_function': 'auto'},\n",
       "  'NeuralNetTorch/d61e8ac0': {'num_epochs': 10,\n",
       "   'epochs_wo_improve': 20,\n",
       "   'activation': 'relu',\n",
       "   'embedding_size_factor': 0.8,\n",
       "   'embed_exponent': 0.56,\n",
       "   'max_embedding_dim': 100,\n",
       "   'y_range': None,\n",
       "   'y_range_extend': 0.05,\n",
       "   'dropout_prob': 0.2430304428163248,\n",
       "   'optimizer': 'adam',\n",
       "   'learning_rate': 0.00012308825223435418,\n",
       "   'weight_decay': 3.1510497250740634e-07,\n",
       "   'proc.embed_min_categories': 1000,\n",
       "   'proc.impute_strategy': 'mean',\n",
       "   'proc.max_category_levels': 100,\n",
       "   'proc.skew_threshold': 1.0,\n",
       "   'use_ngram_features': False,\n",
       "   'num_layers': 2,\n",
       "   'hidden_size': 256,\n",
       "   'max_batch_size': 512,\n",
       "   'use_batchnorm': True,\n",
       "   'loss_function': 'auto'},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                      model  score_val eval_metric  pred_time_val  fit_time  \\\n",
       " 0      WeightedEnsemble_L2      0.380    accuracy       0.040907  7.487318   \n",
       " 1  NeuralNetTorch/e38e9845      0.370    accuracy       0.023070  2.529382   \n",
       " 2  NeuralNetTorch/d61e8ac0      0.355    accuracy       0.011454  0.857236   \n",
       " 3  NeuralNetTorch/1ba3519e      0.355    accuracy       0.016551  2.475698   \n",
       " 4  NeuralNetTorch/4b034927      0.355    accuracy       0.017095  4.578781   \n",
       " 5  NeuralNetTorch/2ed34b4b      0.230    accuracy       0.018821  2.287487   \n",
       " \n",
       "    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                0.000742           0.379155            2       True   \n",
       " 1                0.023070           2.529382            1       True   \n",
       " 2                0.011454           0.857236            1       True   \n",
       " 3                0.016551           2.475698            1       True   \n",
       " 4                0.017095           4.578781            1       True   \n",
       " 5                0.018821           2.287487            1       True   \n",
       " \n",
       "    fit_order  \n",
       " 0          6  \n",
       " 1          4  \n",
       " 2          5  \n",
       " 3          1  \n",
       " 4          2  \n",
       " 5          3  }"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the predictive performance may be poor because we are using few training data points and small ranges for hyperparameters to ensure quick run times. You can call `fit()` multiple times while modifying these settings to better understand how these choices affect performance outcomes. For example: you can increase `subsample_size` to train using a larger dataset, increase the `num_epochs` and `num_boost_round` hyperparameters, and increase the `time_limit` (which you should do for all code in these tutorials). To see more detailed output during the execution of `fit()`, you can also pass in the argument: `verbosity = 3`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a name=\"7\">Model ensembling with stacking/bagging</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Beyond hyperparameter-tuning with a correctly-specified evaluation metric, there are two other methods to boost predictive performance:\n",
    "- bagging and \n",
    "- stack-ensembling\n",
    "\n",
    "You’ll often see performance improve if you specify `num_bag_folds = 5-10`, `num_stack_levels = 1-3` in the call to `fit()`. Beware that doing this will increase training times and memory/disk usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240106_030940\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240106_030940\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Wed Sep 6 21:15:41 UTC 2023\n",
      "CPU Count:          4\n",
      "Memory Avail:       11.02 GB / 15.32 GB (71.9%)\n",
      "Disk Space Avail:   45.03 GB / 49.04 GB (91.8%)\n",
      "===================================================\n",
      "Train Data Rows:    1000\n",
      "Train Data Columns: 14\n",
      "Label Column:       occupation\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Exec-managerial', ' Other-service', ' Craft-repair', ' Sales', ' Prof-specialty', ' Protective-serv', ' ?', ' Adm-clerical', ' Machine-op-inspct', ' Tech-support']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 13 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.996\n",
      "Train Data Class Count: 13\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11283.40 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.55 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('int', ['bool']) : 2 | ['sex', 'class']\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\t0.1155\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\t0.0994\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.01%)\n",
      "\t0.3394\t = Validation score   (accuracy)\n",
      "\t12.64s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t0.3805\t = Validation score   (accuracy)\n",
      "\t6.25s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.03%)\n",
      "\t0.3635\t = Validation score   (accuracy)\n",
      "\t6.91s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ...\n",
      "\t0.3173\t = Validation score   (accuracy)\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ...\n",
      "\t0.3032\t = Validation score   (accuracy)\n",
      "\t0.99s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n",
      "\t0.3715\t = Validation score   (accuracy)\n",
      "\t209.48s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ...\n",
      "\t0.3052\t = Validation score   (accuracy)\n",
      "\t1.01s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ...\n",
      "\t0.3022\t = Validation score   (accuracy)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.05%)\n",
      "\t0.3645\t = Validation score   (accuracy)\n",
      "\t8.17s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.00%)\n",
      "\t0.3795\t = Validation score   (accuracy)\n",
      "\t19.98s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.10%)\n",
      "\t0.3042\t = Validation score   (accuracy)\n",
      "\t11.05s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ...\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
      "\t0.3805\t = Validation score   (accuracy)\n",
      "\t1.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L2 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.07%)\n",
      "\t0.3514\t = Validation score   (accuracy)\n",
      "\t12.11s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.37%)\n",
      "\t0.3614\t = Validation score   (accuracy)\n",
      "\t40.86s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.37%)\n",
      "\t0.3695\t = Validation score   (accuracy)\n",
      "\t73.26s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ...\n",
      "\t0.3484\t = Validation score   (accuracy)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ...\n",
      "\t0.3514\t = Validation score   (accuracy)\n",
      "\t3.87s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.69%)\n",
      "\t0.3795\t = Validation score   (accuracy)\n",
      "\t441.03s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ...\n",
      "\t0.3604\t = Validation score   (accuracy)\n",
      "\t1.02s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ...\n",
      "\t0.3373\t = Validation score   (accuracy)\n",
      "\t0.93s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.70%)\n",
      "\t0.3464\t = Validation score   (accuracy)\n",
      "\t77.28s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.04%)\n",
      "\t0.3655\t = Validation score   (accuracy)\n",
      "\t14.4s\t = Training   runtime\n",
      "\t0.79s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ...\n",
      "\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=1.35%)\n",
      "\t0.3504\t = Validation score   (accuracy)\n",
      "\t236.92s\t = Training   runtime\n",
      "\t0.81s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ...\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.167, 'CatBoost_BAG_L1': 0.146, 'ExtraTreesGini_BAG_L2': 0.125, 'LightGBMXT_BAG_L1': 0.104, 'RandomForestEntr_BAG_L2': 0.104, 'CatBoost_BAG_L2': 0.083, 'KNeighborsDist_BAG_L1': 0.063, 'NeuralNetTorch_BAG_L1': 0.063, 'LightGBM_BAG_L1': 0.042, 'LightGBMXT_BAG_L2': 0.042, 'NeuralNetTorch_BAG_L2': 0.042, 'RandomForestGini_BAG_L1': 0.021}\n",
      "\t0.4016\t = Validation score   (accuracy)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1247.08s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240106_030940\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=\"occupation\", eval_metric=metric).fit(\n",
    "    train_data,\n",
    "    num_bag_folds=5,\n",
    "    num_bag_sets=1,\n",
    "    num_stack_levels=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should not provide `tuning_data` when stacking/bagging, and instead provide all your available data as train_data (which AutoGluon will split in more intelligent ways). Parameter `num_bag_sets` controls how many times the K-fold bagging process is repeated to further reduce variance (increasing this may further boost accuracy but will substantially increase training times, inference latency, and memory/disk usage). Rather than manually searching for good bagging/stacking values yourself, AutoGluon will automatically select good values for you if you specify `auto_stack` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=5\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"agModels-predictOccupation\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Wed Sep 6 21:15:41 UTC 2023\n",
      "CPU Count:          4\n",
      "Memory Avail:       10.70 GB / 15.32 GB (69.8%)\n",
      "Disk Space Avail:   44.46 GB / 49.04 GB (90.7%)\n",
      "===================================================\n",
      "Train Data Rows:    1000\n",
      "Train Data Columns: 14\n",
      "Label Column:       occupation\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Exec-managerial', ' Other-service', ' Craft-repair', ' Sales', ' Prof-specialty', ' Protective-serv', ' ?', ' Adm-clerical', ' Machine-op-inspct', ' Tech-support']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 13 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.996\n",
      "Train Data Class Count: 13\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    10955.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.55 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('int', ['bool']) : 2 | ['sex', 'class']\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 19.91s of the 29.87s of remaining time.\n",
      "\t0.1155\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 19.88s of the 29.84s of remaining time.\n",
      "\t0.0994\t = Validation score   (accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 19.84s of the 29.8s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.01%)\n",
      "\t0.3544\t = Validation score   (accuracy)\n",
      "\t16.5s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Completed 1/5 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.87s of the 9.38s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 1.0}\n",
      "\t0.3544\t = Validation score   (accuracy)\n",
      "\t0.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L2 models ...\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 8.95s of the 8.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.02%)\n",
      "\t0.3484\t = Validation score   (accuracy)\n",
      "\t16.41s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Completed 1/5 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 29.87s of the -12.0s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L2': 0.583, 'NeuralNetFastAI_BAG_L1': 0.417}\n",
      "\t0.3635\t = Validation score   (accuracy)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 42.57s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictOccupation\")\n"
     ]
    }
   ],
   "source": [
    "# Folder where to store trained models\n",
    "save_path = \"agModels-predictOccupation\"\n",
    "\n",
    "predictor = TabularPredictor(label=\"occupation\", eval_metric=metric, path=save_path).fit(\n",
    "    train_data,\n",
    "    auto_stack=True,\n",
    "    time_limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often stacking/bagging will produce superior accuracy than hyperparameter-tuning, but you may try combining both techniques (note: specifying `presets='best_quality'` in `fit()` simply sets `auto_stack=True`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a name=\"8\">Prediction options (inference)</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Even if you’ve started a new Python session since last calling `fit()`, you can still load a previously trained predictor from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `predictor.path` is another way to get the relative path needed to later load predictor.\n",
    "predictor = TabularPredictor.load(\"agModels-predictOccupation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above `save_path` is the same folder previously passed to `TabularPredictor`, in which all the trained models have been saved. You can train easily models on one machine and deploy them on another. Simply copy the `save_path` folder to the new machine and specify its new path in `TabularPredictor.load()`.\n",
    "\n",
    "We can make a prediction on an individual example rather than on a full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Other-service\n",
       "Name: occupation, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select one datapoint to make a prediction\n",
    "datapoint = test_data.iloc[[0]] # Note: .iloc[0] won't work because it returns pandas Series instead of DataFrame\n",
    "\n",
    "predictor.predict(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To output predicted class probabilities instead of predicted classes, you can use:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>?</th>\n",
       "      <th>Adm-clerical</th>\n",
       "      <th>Armed-Forces</th>\n",
       "      <th>Craft-repair</th>\n",
       "      <th>Exec-managerial</th>\n",
       "      <th>Farming-fishing</th>\n",
       "      <th>Handlers-cleaners</th>\n",
       "      <th>Machine-op-inspct</th>\n",
       "      <th>Other-service</th>\n",
       "      <th>Priv-house-serv</th>\n",
       "      <th>Prof-specialty</th>\n",
       "      <th>Protective-serv</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Tech-support</th>\n",
       "      <th>Transport-moving</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.031323</td>\n",
       "      <td>0.164398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.045533</td>\n",
       "      <td>0.04777</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.057586</td>\n",
       "      <td>0.040143</td>\n",
       "      <td>0.326852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027047</td>\n",
       "      <td>0.021433</td>\n",
       "      <td>0.138527</td>\n",
       "      <td>0.032889</td>\n",
       "      <td>0.045299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ?   Adm-clerical   Armed-Forces   Craft-repair   Exec-managerial  \\\n",
       "0  0.031323       0.164398            0.0       0.045533           0.04777   \n",
       "\n",
       "    Farming-fishing   Handlers-cleaners   Machine-op-inspct   Other-service  \\\n",
       "0            0.0212            0.057586            0.040143        0.326852   \n",
       "\n",
       "    Priv-house-serv   Prof-specialty   Protective-serv     Sales  \\\n",
       "0               0.0         0.027047          0.021433  0.138527   \n",
       "\n",
       "    Tech-support   Transport-moving  \n",
       "0       0.032889           0.045299  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a DataFrame that shows which probability corresponds to which class\n",
    "predictor.predict_proba(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `predict()` and `predict_proba()` will utilize the model that AutoGluon thinks is most accurate, which is usually an ensemble of many individual models. Here’s how to see which model this corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26784/850388021.py:1: DeprecationWarning: `get_model_best` has been deprecated and will be removed in version 1.2. Please use `model_best` instead. This will raise an error in the future!\n",
      "  predictor.get_model_best()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'WeightedEnsemble_L3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.get_model_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a name=\"10\">Selecting individual models for predictions</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We can specify a particular model to use for predictions (e.g. to reduce inference latency). Note that a ‘model’ in AutoGluon may refer to for example a single Neural Network, a bagged ensemble of many Neural Network copies trained on different training/validation splits, a weighted ensemble that aggregates the predictions of many other models, or a stacked model that operates on predictions output by other models. This is akin to viewing a RandomForest as one ‘model’ when it is in fact an ensemble of many decision trees.\n",
    "\n",
    "\n",
    "Here’s how to specify a particular model to use for prediction instead of AutoGluon’s default model-choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction from KNeighborsUnif_BAG_L1 model:  Adm-clerical\n"
     ]
    }
   ],
   "source": [
    "# index of model to use\n",
    "i = 0\n",
    "model_to_use = predictor.model_names()[i]\n",
    "model_pred = predictor.predict(datapoint, model=model_to_use)\n",
    "print(f\"Prediction from {model_to_use} model: {model_pred.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily access information about the trained predictor or a particular model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26784/558854764.py:1: DeprecationWarning: `get_model_names` has been deprecated and will be removed in version 1.2. Please use `model_names` instead. This will raise an error in the future!\n",
      "  all_models = predictor.get_model_names()\n"
     ]
    }
   ],
   "source": [
    "all_models = predictor.get_model_names()\n",
    "model_to_use = all_models[i]\n",
    "specific_model = predictor._trainer.load_model(model_to_use)\n",
    "\n",
    "# Objects defined below are dicts with information (not printed here as they are quite large):\n",
    "model_info = specific_model.get_info()\n",
    "predictor_information = predictor.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the label columns remains in the `test_data` DataFrame, we can instead use the shorthand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.3435356740710411,\n",
       " 'balanced_accuracy': 0.2272866632388407,\n",
       " 'mcc': 0.2642006777895094}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"11\">Interpretability: Feature importance</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "To better understand our trained predictor, we can estimate the overall importance of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 14 features using 5000 rows with 5 shuffle sets...\n",
      "\t114.36s\t= Expected runtime (22.87s per shuffle set)\n",
      "\t97.93s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>education-num</th>\n",
       "      <td>0.07984</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>3.292747e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.091759</td>\n",
       "      <td>0.067921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>workclass</th>\n",
       "      <td>0.07508</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>6.108404e-09</td>\n",
       "      <td>5</td>\n",
       "      <td>0.077402</td>\n",
       "      <td>0.072758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>0.05256</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>6.321385e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.061802</td>\n",
       "      <td>0.043318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hours-per-week</th>\n",
       "      <td>0.02096</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>3.406869e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.031103</td>\n",
       "      <td>0.010817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>0.01696</td>\n",
       "      <td>0.003528</td>\n",
       "      <td>2.123362e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.024225</td>\n",
       "      <td>0.009695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>0.01072</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>5.455687e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.016584</td>\n",
       "      <td>0.004856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education</th>\n",
       "      <td>0.00424</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>4.256753e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012816</td>\n",
       "      <td>-0.004336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>relationship</th>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>6.563191e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006864</td>\n",
       "      <td>-0.002864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-gain</th>\n",
       "      <td>0.00200</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>4.173702e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>-0.002014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race</th>\n",
       "      <td>0.00108</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>1.418910e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>-0.002940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>native-country</th>\n",
       "      <td>-0.00036</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>7.818466e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>-0.002278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marital-status</th>\n",
       "      <td>-0.00048</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>6.936733e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>-0.004511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital-loss</th>\n",
       "      <td>-0.00064</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>8.543599e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>-0.003066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fnlwgt</th>\n",
       "      <td>-0.00136</td>\n",
       "      <td>0.005395</td>\n",
       "      <td>6.984572e-01</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009749</td>\n",
       "      <td>-0.012469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                importance    stddev       p_value  n  p99_high   p99_low\n",
       "education-num      0.07984  0.005789  3.292747e-06  5  0.091759  0.067921\n",
       "workclass          0.07508  0.001128  6.108404e-09  5  0.077402  0.072758\n",
       "sex                0.05256  0.004489  6.321385e-06  5  0.061802  0.043318\n",
       "hours-per-week     0.02096  0.004926  3.406869e-04  5  0.031103  0.010817\n",
       "class              0.01696  0.003528  2.123362e-04  5  0.024225  0.009695\n",
       "age                0.01072  0.002848  5.455687e-04  5  0.016584  0.004856\n",
       "education          0.00424  0.004165  4.256753e-02  5  0.012816 -0.004336\n",
       "relationship       0.00200  0.002362  6.563191e-02  5  0.006864 -0.002864\n",
       "capital-gain       0.00200  0.001949  4.173702e-02  5  0.006014 -0.002014\n",
       "race               0.00108  0.001952  1.418910e-01  5  0.005100 -0.002940\n",
       "native-country    -0.00036  0.000932  7.818466e-01  5  0.001558 -0.002278\n",
       "marital-status    -0.00048  0.001958  6.936733e-01  5  0.003551 -0.004511\n",
       "capital-loss      -0.00064  0.001178  8.543599e-01  5  0.001786 -0.003066\n",
       "fnlwgt            -0.00136  0.005395  6.984572e-01  5  0.009749 -0.012469"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.feature_importance(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computed via permutation-shuffling, these feature importance scores quantify the drop in predictive performance (of the already trained predictor) when one columns values are randomly shuffled across rows. The top features in this list contribute most to AutoGluon’s accuracy. Features with non-positive importance score hardly contribute to the predictors accuracy, or may even be actively harmful to include in the data (consider removing these features from your data and calling `fit` again). These scores facilitate interpretability of the predictors global behavior (which features it relies on for all predictions) rather than local explanations that only rationalize one particular prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## <a name=\"12\"> Inference Speed: Model distillation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "While computationally-favorable, single individual models will usually have lower accuracy than weighted/stacked/bagged ensembles. Model Distillation offers one way to retain the computational benefits of a single model, while enjoying some of the accuracy-boost that comes with ensembling. The idea is to train the individual model (which we can call the student) to mimic the predictions of the full stack ensemble (the teacher). Like `refit_full()`, the `distill()` function will produce additional models we can opt to use for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training student models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Distilling with teacher='WeightedEnsemble_L3', teacher_preds=soft, augment_method=spunge ...\n",
      "SPUNGE: Augmenting training data with 3980 synthetic samples for distillation...\n",
      "Distilling with each of these student models: ['LightGBM_DSTL', 'RandomForestMSE_DSTL', 'CatBoost_DSTL', 'NeuralNetTorch_DSTL']\n",
      "Fitting 4 L1 models ...\n",
      "Fitting model: LightGBM_DSTL ... Training model for up to 30.0s of the 30.0s of remaining time.\n",
      "\tWarning: Exception caused LightGBM_DSTL to fail during training... Skipping this model.\n",
      "\t\ttrain() got an unexpected keyword argument 'fobj'\n",
      "Fitting model: RandomForestMSE_DSTL ... Training model for up to 29.57s of the 29.57s of remaining time.\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-1.8307\t = Validation score   (-soft_log_loss)\n",
      "\t3.73s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost_DSTL ... Training model for up to 25.52s of the 25.52s of remaining time.\n",
      "\tWarning: Exception caused CatBoost_DSTL to fail during training... Skipping this model.\n",
      "\t\tfeatures data: pandas.DataFrame column 'workclass' has dtype 'category' but is not in  cat_features list\n",
      "Fitting model: NeuralNetTorch_DSTL ... Training model for up to 25.25s of the 25.24s of remaining time.\n",
      "\tWarning: Exception caused NeuralNetTorch_DSTL to fail during training... Skipping this model.\n",
      "\t\tFound array with 0 feature(s) (shape=(4776, 0)) while a minimum of 1 is required.\n",
      "Repeating k-fold bagging: 2/5\n",
      "Repeating k-fold bagging: 3/5\n",
      "Repeating k-fold bagging: 4/5\n",
      "Repeating k-fold bagging: 5/5\n",
      "Completed 5/5 k-fold bagging repeats ...\n",
      "Distilling with each of these student models: ['WeightedEnsemble_L2_DSTL']\n",
      "Fitting model: WeightedEnsemble_L2_DSTL ... Training model for up to 30.0s of the 24.97s of remaining time.\n",
      "\tEnsemble Weights: {'RandomForestMSE_DSTL': 1.0}\n",
      "\tNote: model has different eval_metric than default.\n",
      "\t-1.8307\t = Validation score   (-soft_log_loss)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Distilled model leaderboard:\n",
      "                      model  score_val    eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      RandomForestMSE_DSTL       0.39  soft_log_loss       0.075659  3.734609                0.075659           3.734609            1       True          7\n",
      "1  WeightedEnsemble_L2_DSTL       0.39  soft_log_loss       0.075960  3.738180                0.000300           0.003571            2       True          8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RandomForestMSE_DSTL', 'WeightedEnsemble_L2_DSTL']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify much longer time limit in real applications\n",
    "student_models = predictor.distill(time_limit=30, verbosity=0)\n",
    "student_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions from RandomForestMSE_DSTL: [' Other-service', ' Farming-fishing', ' Sales', ' Sales', ' Handlers-cleaners']\n"
     ]
    }
   ],
   "source": [
    "preds_student = predictor.predict(test_data, model=student_models[0])\n",
    "print(f\"predictions from {student_models[0]}: {list(preds_student)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding models\n",
    "\n",
    "Finally, you may also exclude specific unwieldy models from being trained at all. Below we exclude models that tend to be slower (K Nearest Neighbors, Neural Network, models with custom larger-than-default hyperparameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240106_033311\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ... Time limit = 30s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240106_033311\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.10.13\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Wed Sep 6 21:15:41 UTC 2023\n",
      "CPU Count:          4\n",
      "Memory Avail:       9.89 GB / 15.32 GB (64.5%)\n",
      "Disk Space Avail:   44.20 GB / 49.04 GB (90.1%)\n",
      "===================================================\n",
      "Train Data Rows:    1000\n",
      "Train Data Columns: 14\n",
      "Label Column:       occupation\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == object).\n",
      "\tFirst 10 (of 15) unique label values:  [' Exec-managerial', ' Other-service', ' Craft-repair', ' Sales', ' Prof-specialty', ' Protective-serv', ' ?', ' Adm-clerical', ' Machine-op-inspct', ' Tech-support']\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       multiclass\n",
      "Preprocessing data ...\n",
      "Warning: Some classes in the training set have fewer than 10 examples. AutoGluon will only keep 13 out of 15 classes for training and will not try to predict the rare classes. To keep more classes, increase the number of datapoints from these rare classes in the training data or reduce label_count_threshold.\n",
      "Fraction of data from classes with at least 10 examples that will be kept for training models: 0.996\n",
      "Train Data Class Count: 13\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    10127.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.55 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n",
      "\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n",
      "\t\t('int', ['bool']) : 2 | ['sex', 'class']\n",
      "\t0.1s = Fit runtime\n",
      "\t14 features in original data used to generate 14 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.2, Train Rows: 796, Val Rows: 200\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 29.89s of the 29.89s of remaining time.\n",
      "\t0.37\t = Validation score   (accuracy)\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 28.79s of the 28.79s of remaining time.\n",
      "\t0.385\t = Validation score   (accuracy)\n",
      "\t1.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 27.2s of the 27.2s of remaining time.\n",
      "\t0.37\t = Validation score   (accuracy)\n",
      "\t2.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestGini ... Training model for up to 24.87s of the 24.87s of remaining time.\n",
      "\t0.355\t = Validation score   (accuracy)\n",
      "\t0.98s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr ... Training model for up to 23.74s of the 23.74s of remaining time.\n",
      "\t0.355\t = Validation score   (accuracy)\n",
      "\t0.93s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 22.66s of the 22.66s of remaining time.\n",
      "\tRan out of time, early stopping on iteration 269.\n",
      "\t0.39\t = Validation score   (accuracy)\n",
      "\t22.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 29.89s of the 0.05s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.364, 'NeuralNetFastAI': 0.205, 'LightGBM': 0.205, 'RandomForestEntr': 0.114, 'LightGBMXT': 0.091, 'RandomForestGini': 0.023}\n",
      "\t0.415\t = Validation score   (accuracy)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 30.34s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240106_033311\")\n"
     ]
    }
   ],
   "source": [
    "excluded_model_types = [\"KNN\", \"NN\", \"custom\"]\n",
    "predictor_light = TabularPredictor(label=\"occupation\", eval_metric=metric).fit(\n",
    "    train_data, excluded_model_types=excluded_model_types, time_limit=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <a name=\"13\">Before You Go</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "After you are done with this Demo, clean model artifacts by uncommenting and executing the cell below.\n",
    "\n",
    "__It is always good practice to clean everything when you are done, preventing the disk from getting full.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r AutogluonModels\n",
    "# !rm -r agModels-predictOccupation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
